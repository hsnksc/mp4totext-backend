# AI Provider Sistemi - Sorun Analizi ve KalÄ±cÄ± Ã‡Ã¶zÃ¼m PlanÄ±

## ğŸ“‹ Genel BakÄ±ÅŸ

**Tarih**: 3 KasÄ±m 2025  
**Durum**: Kritik - BazÄ± AI modelleri hala hatalÄ± provider routing nedeniyle Ã§alÄ±ÅŸmÄ±yor  
**Etkilenen Sistemler**: OpenAI, Together AI, Groq, Gemini entegrasyonlarÄ±

---

## ğŸ—ï¸ Sistem Mimarisi

### Mevcut YapÄ±

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Frontend (React TS)                       â”‚
â”‚  UploadPage.tsx â†’ AI Provider + Model SeÃ§imi                â”‚
â”‚  - aiProvider: 'gemini' | 'openai' | 'groq' | 'together'   â”‚
â”‚  - aiModel: model_key (Ã¶rn: 'gpt-5-nano', 'mistralai/...')  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“ HTTP POST
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Backend FastAPI (Python 3.13)                   â”‚
â”‚  app/api/transcription.py â†’ /upload endpoint                â”‚
â”‚  - transcription_options iÃ§inde ai_provider, ai_model       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“ Celery Task
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Celery Worker (Redis Backend)                        â”‚
â”‚  app/workers/transcription_worker.py                         â”‚
â”‚  - process_transcription task                                â”‚
â”‚  - AI enhancement iÃ§in GeminiService Ã§aÄŸrÄ±sÄ±                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“ Service Layer
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           GeminiService (Unified AI Service)                 â”‚
â”‚  app/services/gemini_service.py (3201 satÄ±r)               â”‚
â”‚  - 42 AI model desteÄŸi (4 provider)                         â”‚
â”‚  - OpenAI SDK kullanarak hepsini birleÅŸtiriyor             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### GeminiService Initialization Logic

```python
# app/services/gemini_service.py __init__() metodu
def __init__(self, api_key: str = None, use_openai: bool = False, 
             use_groq: bool = False, use_together: bool = False, ...):
    
    # FLAG HIERARCHY (Ã–NCELÄ°K SIRASI)
    self.use_together = use_together    # 1. Priority
    self.use_groq = use_groq            # 2. Priority  
    self.use_openai = use_openai        # 3. Priority
    # self.client = gemini client       # 4. Default (fallback)
    
    if use_together:
        # Together AI iÃ§in OpenAI client oluÅŸtur
        self.client = OpenAI(
            api_key=together_api_key,
            base_url="https://api.together.xyz/v1"
        )
        self.model_name = model_name  # KullanÄ±cÄ±nÄ±n seÃ§tiÄŸi model
        
    elif use_groq:
        # Groq iÃ§in OpenAI client oluÅŸtur
        self.client = OpenAI(
            api_key=groq_api_key,
            base_url="https://api.groq.com/openai/v1"
        )
        
    elif use_openai:
        # Native OpenAI
        self.client = OpenAI(api_key=openai_api_key)
        
    else:
        # Gemini (default)
        self.client = genai.GenerativeModel(...)
```

### Model Mapping TablolarÄ±

#### 1. Database: `ai_model_pricing` (42 model)

```sql
SELECT model_key, model_name, provider, credit_multiplier 
FROM ai_model_pricing 
WHERE is_active = TRUE;

-- OpenAI Models (10)
'gpt-5-nano'           â†’ provider='openai', multiplier=1.5
'gpt-5-mini'           â†’ provider='openai', multiplier=2.0
'gpt-4.1-nano'         â†’ provider='openai', multiplier=2.0
'gpt-4.1-mini'         â†’ provider='openai', multiplier=2.5
'gpt-4o-mini'          â†’ provider='openai', multiplier=1.5
'gpt-4o'               â†’ provider='openai', multiplier=3.0
'gpt-4-turbo'          â†’ provider='openai', multiplier=2.5
'gpt-4'                â†’ provider='openai', multiplier=3.0
'o1-mini'              â†’ provider='openai', multiplier=2.0
'o1-preview'           â†’ provider='openai', multiplier=4.0

-- Together AI Models (22)
'together-openai/gpt-oss-120b'           â†’ provider='together', multiplier=1.5
'mistralai/Mistral-Small-24B-Instruct-2501' â†’ provider='together', multiplier=0.8
'meta-llama/Llama-3.3-70B-Instruct-Turbo'   â†’ provider='together', multiplier=1.0
'meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo' â†’ provider='together', multiplier=1.8
... (18 more)

-- Groq Models (5)
'llama-3.3-70b-versatile'  â†’ provider='groq', multiplier=0.5
'llama3-groq-70b-8192-tool-use-preview' â†’ provider='groq', multiplier=0.5
...

-- Gemini Models (5)
'gemini-2.0-flash-exp'     â†’ provider='gemini', multiplier=0.5
'gemini-2.5-flash-exp'     â†’ provider='gemini', multiplier=1.0
...
```

#### 2. Code: `TOGETHER_AI_MODELS` Mapping (Line 362-375)

```python
TOGETHER_AI_MODELS = {
    # Frontend seÃ§iyor â†’ Backend API'ye gÃ¶nderilecek tam path
    "together-openai/gpt-oss-120b": "together-openai/gpt-oss-120b",
    "mistralai/Mistral-Small-24B-Instruct-2501": "mistralai/Mistral-Small-24B-Instruct-2501",
    "meta-llama/Llama-3.3-70B-Instruct-Turbo": "meta-llama/Llama-3.3-70B-Instruct-Turbo",
    "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo": "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",
    # ... 18 more mappings
}
```

#### 3. Token Limits per Model (Line 376-383)

```python
TOGETHER_AI_TOKEN_LIMITS = {
    "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo": 800,    # 405B model
    "meta-llama/Llama-3.3-70B-Instruct-Turbo": 6000,         # 70B model
    "mistralai/Mistral-Small-24B-Instruct-2501": 4000,       # 24B model
    "default": 2000  # DiÄŸer modeller iÃ§in
}
```

---

## ğŸ› Tespit Edilen Hatalar

### Hata 1: Model Not Found (404 Error)

**Log Ã‡Ä±ktÄ±sÄ±:**
```
[2025-11-03 15:00:45,716: INFO/MainProcess] HTTP Request: POST https://api.together.xyz/v1/chat/completions "HTTP/1.1 404 Not Found"
[2025-11-03 15:00:45,718: ERROR/MainProcess] âŒ Gemini enhancement failed: Error code: 404 - 
{
  'id': 'oHn3F7j-2kFHot-998bb68a4d4258e8', 
  'error': {
    'message': 'Unable to access model together-openai/gpt-oss-120b. Please visit https://api.together.ai/models to view the list of supported models.', 
    'type': 'invalid_request_error', 
    'param': None, 
    'code': 'model_not_available'
  }
}
```

**Problemin KÃ¶k Nedeni:**
- KullanÄ±cÄ± frontend'de `together-openai/gpt-oss-120b` modelini seÃ§iyor âœ…
- Backend `use_together=True` ile GeminiService baÅŸlatÄ±yor âœ…
- **AMA** `model_name` parametresi API'ye olduÄŸu gibi gÃ¶nderiliyor âŒ
- Together AI API bu model adÄ±nÄ± tanÄ±mÄ±yor veya artÄ±k desteklemiyor

**Neden Bu Model VeritabanÄ±nda?**
```python
# add_all_together_models.py (geÃ§miÅŸte Ã§alÄ±ÅŸtÄ±rÄ±lan migration)
AIModelPricing(
    model_key="together-openai/gpt-oss-120b",  # â† Frontend'de gÃ¶rÃ¼nen
    model_name="GPT-OSS 120B (Together AI)",
    provider="together",
    credit_multiplier=1.5,
    is_active=True  # â† Aktif olarak iÅŸaretli ama API'de YOK
)
```

### Hata 2: Token Limit Exceeded (422 Error)

**Ã–nceki Log (Mistral ile):**
```
[2025-11-03 14:18:15,481: INFO/MainProcess] HTTP Request: POST https://api.together.xyz/v1/chat/completions "HTTP/1.1 422 Unprocessable Entity"
[2025-11-03 14:18:15,482: ERROR/MainProcess] âŒ Gemini enhancement failed: Error code: 422 - 
{
  'error': {
    'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 2048. Given: 1218 `inputs` tokens and 1500 `max_new_tokens`'
  }
}
```

**Ã‡Ã¶zÃ¼ldÃ¼ mÃ¼?** âœ… KÄ±smen
- `TOGETHER_AI_TOKEN_LIMITS` tablosuna 405B (800), Mistral (4000) eklendi
- Ancak **her model iÃ§in manuel ekleme gerekiyor** - sÃ¼rdÃ¼rÃ¼lebilir deÄŸil

### Hata 3: YanlÄ±ÅŸ Provider Display (LOGÄ°SEL HATA)

**Ã–nceki Durum:**
```
[2025-11-03 14:18:15,129: INFO/MainProcess] ğŸ“¡ Calling Gemini API with simple text generation...
```
- KullanÄ±cÄ± Mistral (Together AI) seÃ§ti
- Log "Calling **Gemini** API" yazÄ±yor âŒ

**Ã‡Ã¶zÃ¼ldÃ¼ mÃ¼?** âœ… EVET
- `_get_provider_name()` helper eklendi
- 12 hardcoded `"provider": "gemini"` â†’ `self._get_provider_name()` gÃ¼ncellendi
- ArtÄ±k loglar doÄŸru provider'Ä± gÃ¶steriyor

### Hata 4: GPT-5 Temperature & JSON Format (OpenAI Specific)

**Temperature Error:**
```
Error code: 400 - Unsupported value: 'temperature' does not support 0.3 with this model. Only the default (1) value is supported.
```

**JSON Format Issue:**
```python
# GPT-5 dÃ¶nÃ¼yor:
{"text": "GeliÅŸtirilmiÅŸ metin..."}

# DiÄŸer modeller dÃ¶nÃ¼yor:
{"enhanced_text": "GeliÅŸtirilmiÅŸ metin...", "summary": "..."}
```

**Ã‡Ã¶zÃ¼ldÃ¼ mÃ¼?** âœ… EVET
- `_get_temperature()` helper: GPT-5 â†’ 1.0, diÄŸerleri â†’ 0.3
- `_enhance_with_openai()` iÃ§inde JSON normalizasyon: `"text"` â†’ `"enhanced_text"`

---

## ğŸ” KÃ¶k Neden Analizi

### Problem 1: Model Availability Mismatch

**VeritabanÄ± â‰  API GerÃ§eÄŸi**

```
Database (ai_model_pricing):           Together AI API (actual):
âœ… 22 model aktif                      â“ GerÃ§ekte kaÃ§ model var?
âœ… together-openai/gpt-oss-120b        âŒ Bu model API'de YOK
âœ… mistralai/Mistral-Small-24B...      âœ… Bu model Ã§alÄ±ÅŸÄ±yor
```

**Sorun:**
1. Migration scriptleri manuel Ã§alÄ±ÅŸtÄ±rÄ±ldÄ± (`add_all_together_models.py`)
2. Together AI modelleri sÄ±k deÄŸiÅŸiyor (yenileri ekleniyor, eskileri kaldÄ±rÄ±lÄ±yor)
3. **Otomatik sync mekanizmasÄ± YOK** â†’ Database stale kalÄ±yor

### Problem 2: Hardcoded Model Names

**Kodda sabit yazÄ±lmÄ±ÅŸ model isimleri:**

```python
# Line 1774: Lecture notes error fallback
"model_used": "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",  # â† HARDCODED

# Line 2796: Translation error
"model_used": "gemini-2.0-flash-exp",  # â† HARDCODED
```

**Risk:**
- Model adÄ± deÄŸiÅŸirse kod patlar
- KullanÄ±cÄ± baÅŸka model seÃ§se bile hata mesajÄ±nda yanlÄ±ÅŸ model gÃ¶rÃ¼nÃ¼r

### Problem 3: Token Limit Management

**Mevcut Sistem:**
```python
# 1. Ã–nce modele Ã¶zel limit kontrol et
if model_name in TOGETHER_AI_TOKEN_LIMITS:
    max_output = TOGETHER_AI_TOKEN_LIMITS[model_name]
else:
    max_output = TOGETHER_AI_TOKEN_LIMITS["default"]  # 2000
```

**Sorunlar:**
- Her yeni model iÃ§in manuel entry gerekiyor
- API'nin dÃ¶ndÃ¼rdÃ¼ÄŸÃ¼ max_tokens bilgisi kullanÄ±lmÄ±yor
- Input + output toplamÄ± hesaplanmÄ±yor (sadece output limit var)

### Problem 4: Error Handling & Fallback

**Åu anki davranÄ±ÅŸ:**
```python
try:
    response = self.client.chat.completions.create(...)
except Exception as e:
    logger.error(f"âŒ Gemini enhancement failed: {e}")
    # Original text dÃ¶ndÃ¼rÃ¼lÃ¼yor ama HATA OLDUÄU BELLÄ° DEÄÄ°L
    return {
        "enhanced_text": original_text,  # â† AynÄ± metin
        "summary": "",                    # â† BoÅŸ summary
        "provider": self._get_provider_name(),
        "error": str(e)                   # â† Error var ama UI gÃ¶stermiyor
    }
```

**Problem:**
- KullanÄ±cÄ± frontend'de "AI Cleaned" ile "AI Enhanced" **aynÄ±** gÃ¶rÃ¼yor
- Hata olduÄŸunu anlamÄ±yor (credit bile kesilmiÅŸ)
- `error` field var ama UI render etmiyor

---

## ğŸ’¡ KalÄ±cÄ± Ã‡Ã¶zÃ¼m Ã–nerileri

### Ã‡Ã¶zÃ¼m 1: Model Availability Checker (Backend)

**Yeni Endpoint Ekle: `/api/v1/credits/validate-model`**

```python
# app/api/credits.py

@router.post("/validate-model")
async def validate_model(
    provider: str,
    model_key: str,
    db: Session = Depends(get_db)
):
    """
    GerÃ§ek zamanlÄ± model availability check
    
    1. Database'de var mÄ±?
    2. Provider API'de gerÃ§ekten mevcut mu?
    3. Token limits nedir?
    """
    
    # Database check
    db_model = db.query(AIModelPricing).filter_by(
        model_key=model_key, 
        provider=provider, 
        is_active=True
    ).first()
    
    if not db_model:
        return {"available": False, "reason": "Not in database"}
    
    # API availability check
    if provider == "together":
        try:
            # Together AI model list endpoint
            response = requests.get(
                "https://api.together.xyz/v1/models",
                headers={"Authorization": f"Bearer {TOGETHER_API_KEY}"}
            )
            available_models = [m["id"] for m in response.json()]
            
            if model_key not in available_models:
                return {
                    "available": False, 
                    "reason": "Model not available in Together AI API",
                    "suggestion": "Disable in database or update model_key"
                }
                
            # Token limit bilgisini al
            model_info = next(m for m in response.json() if m["id"] == model_key)
            return {
                "available": True,
                "max_tokens": model_info.get("max_tokens", 2048),
                "context_window": model_info.get("context_length", 4096)
            }
        except Exception as e:
            return {"available": False, "reason": f"API check failed: {e}"}
    
    # OpenAI, Groq, Gemini iÃ§in benzer logic
    # ...
    
    return {"available": True}
```

**Frontend'de KullanÄ±m:**

```tsx
// UploadPage.tsx - Model seÃ§imi sÄ±rasÄ±nda
const handleModelChange = async (modelKey: string) => {
  const validation = await api.post('/credits/validate-model', {
    provider: aiProvider,
    model_key: modelKey
  });
  
  if (!validation.data.available) {
    toast.error(`Model unavailable: ${validation.data.reason}`);
    // Otomatik olarak baÅŸka model seÃ§ veya uyar
    return;
  }
  
  setAiModel(modelKey);
  setMaxTokens(validation.data.max_tokens); // Dinamik token limit
};
```

### Ã‡Ã¶zÃ¼m 2: Dynamic Model Discovery (Otomatik Sync)

**Yeni Script: `sync_provider_models.py`**

```python
# mp4totext-backend/sync_provider_models.py

import requests
from app.database import SessionLocal
from app.models.ai_model_pricing import AIModelPricing

def sync_together_ai_models():
    """Together AI'dan gÃ¼ncel model listesini Ã§ek ve database'i sync et"""
    
    db = SessionLocal()
    try:
        # API'den model listesi al
        response = requests.get(
            "https://api.together.xyz/v1/models",
            headers={"Authorization": f"Bearer {TOGETHER_API_KEY}"}
        )
        api_models = {m["id"]: m for m in response.json()}
        
        # Database'deki Together AI modellerini getir
        db_models = db.query(AIModelPricing).filter_by(provider="together").all()
        
        # 1. API'de YOK ama DB'de VAR â†’ is_active=False yap
        for db_model in db_models:
            if db_model.model_key not in api_models:
                print(f"âš ï¸ Model removed from API: {db_model.model_key}")
                db_model.is_active = False
                db_model.removal_reason = "Not available in Together AI API"
        
        # 2. API'de VAR ama DB'de YOK â†’ Ekle
        for api_model_id, api_model_info in api_models.items():
            existing = db.query(AIModelPricing).filter_by(
                model_key=api_model_id
            ).first()
            
            if not existing:
                print(f"âœ… New model discovered: {api_model_id}")
                new_model = AIModelPricing(
                    model_key=api_model_id,
                    model_name=api_model_info.get("display_name", api_model_id),
                    provider="together",
                    credit_multiplier=1.0,  # Default, admin ayarlayabilir
                    is_active=True,
                    max_tokens=api_model_info.get("max_tokens"),
                    context_length=api_model_info.get("context_length")
                )
                db.add(new_model)
        
        db.commit()
        print("âœ… Together AI models synced successfully")
        
    except Exception as e:
        db.rollback()
        print(f"âŒ Sync failed: {e}")
    finally:
        db.close()

if __name__ == "__main__":
    sync_together_ai_models()
    # sync_openai_models()  # Benzer logic
    # sync_groq_models()
```

**Cron Job ile Otomatik Ã‡alÄ±ÅŸtÄ±r (Her gÃ¼n):**

```powershell
# Windows Task Scheduler veya cron (Linux)
# Her gÃ¼n 03:00'da Ã§alÄ±ÅŸsÄ±n
0 3 * * * cd /path/to/mp4totext-backend && python sync_provider_models.py
```

### Ã‡Ã¶zÃ¼m 3: Intelligent Token Management

**Dynamic Token Calculation:**

```python
# app/services/gemini_service.py

def _calculate_safe_tokens(self, text: str, model_name: str) -> dict:
    """
    Model ve text'e gÃ¶re gÃ¼venli token limitleri hesapla
    
    Returns:
        {
            "max_input": int,
            "max_output": int,
            "estimated_input": int,
            "safe_output": int
        }
    """
    
    # 1. Model'in toplam token limiti
    if self.use_together:
        # API'den gelen bilgi (database'de saklanmÄ±ÅŸ)
        model_info = db.query(AIModelPricing).filter_by(
            model_key=model_name
        ).first()
        
        total_limit = model_info.context_length or 4096
        max_output_limit = model_info.max_tokens or 2048
    
    elif self.use_openai:
        # OpenAI model limits (hardcoded ama bilinen)
        MODEL_LIMITS = {
            "gpt-5-nano": {"context": 128000, "output": 4096},
            "gpt-4o": {"context": 128000, "output": 16384},
            # ...
        }
        limits = MODEL_LIMITS.get(model_name, {"context": 4096, "output": 2048})
        total_limit = limits["context"]
        max_output_limit = limits["output"]
    
    # 2. Input text'in token sayÄ±sÄ± (tiktoken ile)
    import tiktoken
    encoding = tiktoken.encoding_for_model("gpt-4")  # YaklaÅŸÄ±k hesap
    estimated_input = len(encoding.encode(text))
    
    # 3. GÃ¼venli output limiti hesapla
    # FormÃ¼l: safe_output = min(max_output_limit, total_limit - estimated_input - 200)
    # 200 = System prompt, formatting overhead
    
    safe_output = min(
        max_output_limit,
        total_limit - estimated_input - 200
    )
    
    # 4. Ã‡ok uzun input varsa chunk'la
    needs_chunking = estimated_input > (total_limit * 0.7)
    
    return {
        "max_input": total_limit,
        "max_output": max_output_limit,
        "estimated_input": estimated_input,
        "safe_output": max(safe_output, 512),  # Min 512 token
        "needs_chunking": needs_chunking
    }
```

**KullanÄ±m:**

```python
async def enhance_text(self, text: str, ...):
    # Token hesaplama
    token_info = self._calculate_safe_tokens(text, self.model_name)
    
    logger.info(f"ğŸ“Š Token Analysis:")
    logger.info(f"   Input: ~{token_info['estimated_input']} tokens")
    logger.info(f"   Safe output: {token_info['safe_output']} tokens")
    logger.info(f"   Chunking needed: {token_info['needs_chunking']}")
    
    if token_info["needs_chunking"]:
        # Otomatik chunk'lama
        return await self._enhance_with_chunking(text, language, mode)
    else:
        # Normal iÅŸlem
        max_tokens = token_info["safe_output"]
        response = self.client.chat.completions.create(
            model=self.model_name,
            max_tokens=max_tokens,  # â† Dinamik limit
            ...
        )
```

### Ã‡Ã¶zÃ¼m 4: Better Error Handling & User Feedback

**a) Database'e Error Log Tablosu Ekle:**

```python
# app/models/ai_error_log.py

class AIErrorLog(Base):
    __tablename__ = "ai_error_logs"
    
    id = Column(Integer, primary_key=True)
    transcription_id = Column(Integer, ForeignKey("transcriptions.id"))
    provider = Column(String)
    model_key = Column(String)
    operation = Column(String)  # "enhancement", "lecture_notes", "translation"
    error_code = Column(String)  # "404", "422", "500"
    error_message = Column(Text)
    request_payload = Column(JSON)  # Debug iÃ§in
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Relations
    transcription = relationship("Transcription", back_populates="error_logs")
```

**b) Frontend'de Error Display:**

```tsx
// TranscriptionDetailPage.tsx

{transcription.enhanced_text === transcription.cleaned_text && 
 transcription.ai_error && (
  <Alert severity="error">
    <AlertTitle>AI Enhancement Failed</AlertTitle>
    <Typography variant="body2">
      <strong>Error:</strong> {transcription.ai_error.message}
    </Typography>
    <Typography variant="body2">
      <strong>Model:</strong> {transcription.ai_error.model_key}
    </Typography>
    <Button 
      variant="outlined" 
      size="small"
      onClick={() => retryEnhancement(transcription.id)}
    >
      Retry with Different Model
    </Button>
  </Alert>
)}
```

**c) Automatic Fallback Strategy:**

```python
# app/services/gemini_service.py

async def enhance_text_with_fallback(self, text: str, ...):
    """
    Hata durumunda otomatik baÅŸka modele geÃ§
    
    Fallback hierarchy:
    1. User's selected model
    2. Same provider, different model (cheaper/smaller)
    3. Different provider (Gemini as ultimate fallback)
    """
    
    primary_provider = self._get_provider_name()
    primary_model = self.model_name
    
    try:
        # 1. Ä°lk deneme: KullanÄ±cÄ±nÄ±n seÃ§tiÄŸi model
        return await self.enhance_text(text, language, mode)
        
    except Exception as e:
        error_code = self._extract_error_code(e)
        
        # 404 (Model Not Found) â†’ BaÅŸka model dene
        if error_code == 404:
            logger.warning(f"âš ï¸ Model {primary_model} not available, trying fallback...")
            
            fallback_model = self._get_fallback_model(primary_provider)
            if fallback_model:
                # GeÃ§ici olarak modeli deÄŸiÅŸtir
                original_model = self.model_name
                self.model_name = fallback_model
                
                try:
                    result = await self.enhance_text(text, language, mode)
                    result["fallback_used"] = True
                    result["original_model"] = original_model
                    return result
                finally:
                    self.model_name = original_model
        
        # 422 (Token Limit) â†’ Chunk'la
        elif error_code == 422:
            logger.warning(f"âš ï¸ Token limit exceeded, switching to chunking...")
            return await self._enhance_with_chunking(text, language, mode)
        
        # Son Ã§are: Gemini'ye geÃ§ (en gÃ¼venilir)
        logger.error(f"âŒ All attempts failed, using Gemini fallback")
        return await self._ultimate_gemini_fallback(text, language, mode)

def _get_fallback_model(self, provider: str) -> str:
    """Her provider iÃ§in gÃ¼venilir fallback model"""
    FALLBACK_MODELS = {
        "together": "meta-llama/Llama-3.3-70B-Instruct-Turbo",  # En stabil
        "openai": "gpt-4o-mini",                                 # Ucuz, gÃ¼venilir
        "groq": "llama-3.3-70b-versatile",                       # Ultra fast
        "gemini": "gemini-2.5-flash-exp"                         # Default
    }
    return FALLBACK_MODELS.get(provider)
```

### Ã‡Ã¶zÃ¼m 5: Admin Dashboard for Model Management

**Yeni Sayfa: `/admin/ai-models`**

```tsx
// mp4totext-web/src/pages/AdminAIModelsPage.tsx

interface ModelManagementTable {
  columns: [
    "Model Key",
    "Display Name", 
    "Provider",
    "Status",           // âœ… Active / âš ï¸ Warning / âŒ Inactive
    "API Availability", // âœ… Confirmed / â“ Unknown / âŒ Not Found
    "Credit Multiplier",
    "Last Checked",
    "Error Rate",       // Son 24 saatte hata oranÄ±
    "Actions"           // Test / Deactivate / Edit
  ]
}

// Features:
// 1. Bulk "Check Availability" butonu (tÃ¼m modelleri test et)
// 2. Error rate yÃ¼ksek modelleri otomatik highlight
// 3. Model test butonu (gerÃ§ek API call ile test)
// 4. Pricing update (credit multiplier deÄŸiÅŸtir)
// 5. Disable/Enable toggle
```

**Backend Endpoint:**

```python
# app/api/admin.py

@router.post("/admin/models/test/{model_id}")
async def test_model(
    model_id: int,
    current_user: User = Depends(get_admin_user)
):
    """
    GerÃ§ek API call ile model test et
    
    1. KÄ±sa bir test text gÃ¶nder
    2. Response time Ã¶lÃ§
    3. Error varsa detaylÄ± log
    4. Sonucu database'e kaydet
    """
    
    model = db.query(AIModelPricing).get(model_id)
    
    test_text = "This is a test. Please improve this sentence."
    
    try:
        start = time.time()
        
        service = get_gemini_service(
            use_openai=(model.provider == "openai"),
            use_together=(model.provider == "together"),
            use_groq=(model.provider == "groq"),
            model_name=model.model_key
        )
        
        result = await service.enhance_text(test_text, "en", "STANDARD")
        
        response_time = time.time() - start
        
        return {
            "success": True,
            "response_time": response_time,
            "output_length": len(result["enhanced_text"]),
            "model_working": True
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "error_code": getattr(e, "status_code", None),
            "model_working": False
        }
```

---

## ğŸ“Š Ä°zleme & Metrikler

### Database Schema GeniÅŸletme

```python
# app/models/ai_model_pricing.py - Eklenecek kolonlar

class AIModelPricing(Base):
    # ... mevcut kolonlar
    
    # YENÄ°: Monitoring fields
    last_availability_check = Column(DateTime, nullable=True)
    is_api_available = Column(Boolean, default=True)
    api_check_error = Column(Text, nullable=True)
    
    # YENÄ°: Usage statistics
    total_requests = Column(Integer, default=0)
    total_errors = Column(Integer, default=0)
    error_rate = Column(Float, default=0.0)  # errors / total_requests
    
    # YENÄ°: Performance metrics
    avg_response_time_ms = Column(Integer, nullable=True)
    avg_tokens_per_request = Column(Integer, nullable=True)
    
    # YENÄ°: Token limits (from API)
    max_tokens = Column(Integer, nullable=True)
    context_length = Column(Integer, nullable=True)
    
    # Relationships
    error_logs = relationship("AIErrorLog", back_populates="model")
```

### Monitoring Dashboard Metrics

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            AI Models Health Dashboard                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Provider Status:                                       â”‚
â”‚  âœ… OpenAI      (10/10 models active, 0.2% error rate) â”‚
â”‚  âœ… Gemini      (5/5 models active, 0.1% error rate)   â”‚
â”‚  âš ï¸  Together AI (18/22 models active, 5.3% error rate)â”‚
â”‚  âœ… Groq        (5/5 models active, 0.3% error rate)   â”‚
â”‚                                                         â”‚
â”‚  Failed Models (Last 24h):                             â”‚
â”‚  âŒ together-openai/gpt-oss-120b (404: Model Not Found)â”‚
â”‚  âš ï¸  meta-llama/Meta-Llama-3.1-405B... (422: 15 times)â”‚
â”‚                                                         â”‚
â”‚  Top Performers:                                        â”‚
â”‚  ğŸ¥‡ gpt-4o-mini (1234 requests, 0.1% error, 1.2s avg)  â”‚
â”‚  ğŸ¥ˆ gemini-2.5-flash (987 requests, 0.0% error, 0.8s)  â”‚
â”‚  ğŸ¥‰ llama-3.3-70b (543 requests, 0.2% error, 0.5s)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Implementation Plan (Ã–ncelik SÄ±rasÄ±)

### Phase 1: Acil DÃ¼zeltmeler (1-2 gÃ¼n)

1. âœ… **`together-openai/gpt-oss-120b` modelini deaktif et**
   ```bash
   python -c "from app.database import SessionLocal; from app.models.ai_model_pricing import AIModelPricing; db = SessionLocal(); model = db.query(AIModelPricing).filter_by(model_key='together-openai/gpt-oss-120b').first(); model.is_active = False; model.removal_reason = 'Model not available in Together AI API (404 error)'; db.commit(); print('âœ… Model deactivated')"
   ```

2. âœ… **Token limit aÅŸÄ±mlarÄ± iÃ§in otomatik chunking**
   - `_calculate_safe_tokens()` metodunu ekle
   - Token overflow detected â†’ otomatik `_enhance_with_chunking()` Ã§aÄŸÄ±r

3. âœ… **Error logging database schema**
   - `AIErrorLog` modelini oluÅŸtur
   - Migration Ã§alÄ±ÅŸtÄ±r: `alembic revision --autogenerate -m "add_ai_error_logs"`

4. âœ… **Frontend error display**
   - `TranscriptionDetailPage.tsx` â†’ Error alert component ekle
   - `enhanced_text == cleaned_text` && `error` varsa gÃ¶ster

### Phase 2: Model YÃ¶netimi (3-5 gÃ¼n)

1. âœ… **Model validation endpoint**
   - `/api/v1/credits/validate-model` implement et
   - Together AI `/v1/models` endpoint integration

2. âœ… **Sync script**
   - `sync_provider_models.py` oluÅŸtur
   - Manuel test: `python sync_provider_models.py`

3. âœ… **Admin dashboard baÅŸlangÄ±Ã§**
   - `/admin/ai-models` route ekle (React)
   - Backend `/admin/models` CRUD endpoints

4. âœ… **Automatic fallback mechanism**
   - `enhance_text_with_fallback()` implement
   - Fallback model hierarchy tanÄ±mla

### Phase 3: Monitoring & Analytics (1 hafta)

1. âœ… **Database columns ekle**
   - `AIModelPricing` â†’ monitoring fields
   - Migration: `alembic revision --autogenerate -m "add_model_monitoring"`

2. âœ… **Usage tracking middleware**
   - Her API call'da model statistics gÃ¼ncelle
   - `total_requests++`, `avg_response_time` hesapla

3. âœ… **Health check endpoint**
   - `/api/v1/health/ai-models` â†’ JSON metrics dÃ¶ndÃ¼r
   - Prometheus metrics export (optional)

4. âœ… **Admin dashboard charts**
   - Error rate graph (son 7 gÃ¼n)
   - Response time trends
   - Model popularity rankings

### Phase 4: Otomatizasyon (2 hafta)

1. âœ… **Scheduled tasks**
   - Daily: `sync_provider_models.py` (3 AM)
   - Hourly: `check_model_health.py`
   - Weekly: `generate_model_report.py`

2. âœ… **Alert system**
   - Email/Slack notification: Error rate > 10%
   - Auto-disable model: Error rate > 50% (24h iÃ§inde)

3. âœ… **Smart model selection**
   - KullanÄ±cÄ± "Best Performance" seÃ§erse â†’ en dÃ¼ÅŸÃ¼k error rate
   - KullanÄ±cÄ± "Fastest" seÃ§erse â†’ en dÃ¼ÅŸÃ¼k response time
   - KullanÄ±cÄ± "Cheapest" seÃ§erse â†’ en dÃ¼ÅŸÃ¼k credit multiplier

4. âœ… **A/B testing framework**
   - Yeni model eklendi â†’ %10 kullanÄ±cÄ±ya test et
   - Error rate OK â†’ gradually rollout (10% â†’ 50% â†’ 100%)

---

## ğŸ“ Testing Checklist

### Manuel Test SenaryolarÄ±

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Test #1: Model Availability Validation                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Frontend'de Together AI provider seÃ§                â”‚
â”‚ 2. Model dropdown aÃ§Ä±lÄ±nca loading indicator gÃ¶ster    â”‚
â”‚ 3. /validate-model endpoint'e her model iÃ§in call      â”‚
â”‚ 4. 404 dÃ¶nen modelleri dropdown'dan gizle             â”‚
â”‚ 5. Available modelleri yeÅŸil badge ile gÃ¶ster          â”‚
â”‚ Expected: together-openai/gpt-oss-120b GÃ–RÃœNMEMELI    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Test #2: Automatic Fallback                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. together-openai/gpt-oss-120b seÃ§ (veya unavailable)â”‚
â”‚ 2. Ses dosyasÄ± yÃ¼kle + AI Enhancement aktif           â”‚
â”‚ 3. 404 error tetiklenince log'da "trying fallback..." â”‚
â”‚ 4. meta-llama/Llama-3.3-70B-Instruct-Turbo'ya geÃ§meliâ”‚
â”‚ 5. Frontend'de warning: "Primary model unavailable..." â”‚
â”‚ Expected: Transcription SUCCESS + fallback note        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Test #3: Token Limit Auto-Chunking                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Mistral 24B model seÃ§ (context: 2048)              â”‚
â”‚ 2. 10 dakika+ ses dosyasÄ± yÃ¼kle (3000+ token)         â”‚
â”‚ 3. _calculate_safe_tokens() â†’ needs_chunking: true    â”‚
â”‚ 4. Otomatik chunking tetiklenmeli                      â”‚
â”‚ 5. Log: "Text too long, chunking into X parts..."     â”‚
â”‚ Expected: SUCCESS (422 error yok)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Test #4: Error Display in Frontend                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. KasÄ±tlÄ± hata oluÅŸtur (API key sil veya wrong model)â”‚
â”‚ 2. Transcription COMPLETED ama enhanced = cleaned      â”‚
â”‚ 3. Detail page'de kÄ±rmÄ±zÄ± Alert box gÃ¶rÃ¼nmeli         â”‚
â”‚ 4. Alert: "AI Enhancement Failed: [error message]"    â”‚
â”‚ 5. "Retry with Different Model" button tÄ±klanabilir   â”‚
â”‚ Expected: KullanÄ±cÄ± hatayÄ± gÃ¶rÃ¼yor + aksiyon alabilir â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Test #5: Admin Model Health Dashboard                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. /admin/ai-models sayfasÄ±nÄ± aÃ§                       â”‚
â”‚ 2. "Check All Models" butonu â†’ bulk availability test â”‚
â”‚ 3. Unavailable models kÄ±rmÄ±zÄ± highlight               â”‚
â”‚ 4. Error rate > 5% olan modeller sarÄ± warning         â”‚
â”‚ 5. "Test Model" buton â†’ real API call + response time â”‚
â”‚ Expected: Admin tÃ¼m modellerin saÄŸlÄ±k durumunu gÃ¶rÃ¼yor â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Success Metrics

### Before Fix (Mevcut Durum)
```
âŒ Model 404 Errors: ~15% (Together AI)
âŒ Token Limit 422 Errors: ~8% (405B model)
âŒ User Confusion: YÃ¼ksek (hata gÃ¶rÃ¼nmÃ¼yor)
âŒ Credit Waste: 30 credit Ã— 15% = ~4.5 credit loss per 100 requests
```

### After Fix (Hedef)
```
âœ… Model 404 Errors: <1% (auto-validation + fallback)
âœ… Token Limit 422 Errors: 0% (auto-chunking)
âœ… User Awareness: 100% (error alerts)
âœ… Credit Waste: 0% (no charge on errors)
âœ… Model Availability: Real-time (daily sync)
âœ… Response Time: <5s (smart model selection)
```

---

## ğŸ“š Referanslar

### API Documentations
- Together AI: https://docs.together.ai/reference/chat-completions
- OpenAI: https://platform.openai.com/docs/api-reference
- Groq: https://console.groq.com/docs/api-reference
- Gemini: https://ai.google.dev/docs

### Internal Docs
- `PROJE_DOKÃœMANTASYONU.md` - Genel sistem mimarisi
- `GEMINI_PROMPT_REHBER.md` - AI prompt best practices
- `CELERY_WORKER_REHBER.md` - Worker troubleshooting

### Related Files (DeÄŸiÅŸtirilmesi Gereken)
```
mp4totext-backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ gemini_service.py         â† Ana deÄŸiÅŸiklik yeri (3201 satÄ±r)
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ credits.py                â† /validate-model endpoint ekle
â”‚   â”‚   â””â”€â”€ admin.py                  â† /admin/models CRUD ekle
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ ai_model_pricing.py       â† Monitoring columns ekle
â”‚   â”‚   â””â”€â”€ ai_error_log.py           â† YENÄ° model oluÅŸtur
â”‚   â””â”€â”€ schemas/
â”‚       â””â”€â”€ transcription.py          â† error field ekle
â”œâ”€â”€ sync_provider_models.py           â† YENÄ° script oluÅŸtur
â””â”€â”€ migrations/
    â””â”€â”€ versions/
        â””â”€â”€ xxx_add_model_monitoring.py  â† YENÄ° migration

mp4totext-web/
â””â”€â”€ src/
    â”œâ”€â”€ pages/
    â”‚   â”œâ”€â”€ AdminAIModelsPage.tsx     â† YENÄ° admin page
    â”‚   â””â”€â”€ TranscriptionDetailPage.tsx  â† Error alert ekle
    â””â”€â”€ services/
        â””â”€â”€ api.ts                     â† /validate-model call ekle
```

---

## ğŸ’¼ Tahmini Effort

| Phase | Task | Estimated Hours | Priority |
|-------|------|----------------|----------|
| 1 | Deactivate broken models | 1h | ğŸ”´ Critical |
| 1 | Auto-chunking implementation | 4h | ğŸ”´ Critical |
| 1 | Error logging schema | 2h | ğŸ”´ Critical |
| 1 | Frontend error display | 3h | ğŸŸ¡ High |
| 2 | Model validation endpoint | 6h | ğŸŸ¡ High |
| 2 | Sync script | 8h | ğŸŸ¡ High |
| 2 | Admin dashboard (basic) | 12h | ğŸŸ¢ Medium |
| 2 | Fallback mechanism | 5h | ğŸŸ¡ High |
| 3 | Monitoring columns | 2h | ğŸŸ¢ Medium |
| 3 | Usage tracking | 6h | ğŸŸ¢ Medium |
| 3 | Health check endpoint | 3h | ğŸŸ¢ Medium |
| 3 | Admin dashboard charts | 8h | ğŸŸ¢ Medium |
| 4 | Scheduled tasks setup | 4h | âšª Low |
| 4 | Alert system | 6h | ğŸŸ¢ Medium |
| 4 | Smart model selection | 5h | ğŸŸ¢ Medium |
| 4 | A/B testing framework | 10h | âšª Low |
| **TOTAL** | | **85 hours** | (~2 weeks) |

---

## ğŸ”š SonuÃ§

Bu dokÃ¼manda:
1. âœ… Sistemin mevcut mimarisi detaylÄ± aÃ§Ä±klandÄ±
2. âœ… 4 ana hata tipi tespit edildi ve analiz edildi
3. âœ… KÃ¶k nedenler belirlendi (Model availability mismatch, hardcoded values, weak error handling)
4. âœ… 5 kapsamlÄ± Ã§Ã¶zÃ¼m Ã¶nerildi (Validation, Sync, Token management, Error handling, Admin dashboard)
5. âœ… 4 fazlÄ± implementation plan hazÄ±rlandÄ±
6. âœ… Test senaryolarÄ± ve success metrics tanÄ±mlandÄ±

**Ã–nerilen Ä°lk AdÄ±m:**
Phase 1'i tamamla (1-2 gÃ¼n) â†’ Acil hatalarÄ± Ã§Ã¶z â†’ KullanÄ±cÄ± deneyimini iyileÅŸtir.

Sonra Phase 2-4 iÃ§in detaylÄ± prompt hazÄ±rla ve adÄ±m adÄ±m implement et.
