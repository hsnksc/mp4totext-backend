version: '3.8'

# MP4toText Backend - Production Environment
# Scalable architecture for 1000+ concurrent users
# Usage: docker-compose -f docker-compose.prod.yml up -d

services:
  # =============================================================================
  # REDIS - Message Broker & Result Backend (Production)
  # =============================================================================
  redis:
    image: redis:7-alpine
    container_name: mp4totext-redis-prod
    command: >
      redis-server
      --requirepass ${REDIS_PASSWORD}
      --appendonly yes
      --maxmemory 2gb
      --maxmemory-policy allkeys-lru
      --save 900 1
      --save 300 10
      --save 60 10000
    volumes:
      - redis_prod_data:/data
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "--no-auth-warning", "-a", "${REDIS_PASSWORD}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: always
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    networks:
      - mp4totext_prod
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # =============================================================================
  # CELERY WORKER - CRITICAL QUEUE (Upload, Real-time ops)
  # Scale: 3 replicas with autoscale 6-2
  # =============================================================================
  celery_worker_critical:
    build:
      context: .
      dockerfile: Dockerfile.prod
    command: >
      celery -A app.celery_app worker
      -Q critical
      -n worker_critical@%h
      --loglevel=warning
      --autoscale=6,2
      --max-tasks-per-child=1000
    volumes:
      - upload_temp:/tmp/mp4totext_uploads
    environment:
      - ENVIRONMENT=production
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - PYTHONUNBUFFERED=1
      - PYTHONOPTIMIZE=1
    env_file:
      - .env.production
    depends_on:
      redis:
        condition: service_healthy
    restart: always
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
    networks:
      - mp4totext_prod
    logging:
      driver: "json-file"
      options:
        max-size: "20m"
        max-file: "5"

  # =============================================================================
  # CELERY WORKER - HIGH QUEUE (Transcription with Whisper)
  # Scale: 5 replicas with autoscale 12-4
  # Heavy workload: GPU-enabled if available
  # =============================================================================
  celery_worker_high:
    build:
      context: .
      dockerfile: Dockerfile.prod
    command: >
      celery -A app.celery_app worker
      -Q high
      -n worker_high@%h
      --loglevel=warning
      --autoscale=12,4
      --max-tasks-per-child=500
      --prefetch-multiplier=1
    volumes:
      - upload_temp:/tmp/mp4totext_uploads
      - model_cache:/root/.cache/whisper
    environment:
      - ENVIRONMENT=production
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - PYTHONUNBUFFERED=1
      - PYTHONOPTIMIZE=1
      - WHISPER_DEVICE=cpu
      - FASTER_WHISPER_DEVICE=auto
    env_file:
      - .env.production
    depends_on:
      redis:
        condition: service_healthy
    restart: always
    deploy:
      replicas: 5
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          cpus: '2'
          memory: 4G
      update_config:
        parallelism: 2
        delay: 10s
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
    networks:
      - mp4totext_prod
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"

  # =============================================================================
  # CELERY WORKER - DEFAULT QUEUE (AI Enhancement)
  # Scale: 4 replicas with autoscale 10-3
  # =============================================================================
  celery_worker_default:
    build:
      context: .
      dockerfile: Dockerfile.prod
    command: >
      celery -A app.celery_app worker
      -Q default
      -n worker_default@%h
      --loglevel=warning
      --autoscale=10,3
      --max-tasks-per-child=1000
    environment:
      - ENVIRONMENT=production
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - PYTHONUNBUFFERED=1
      - PYTHONOPTIMIZE=1
    env_file:
      - .env.production
    depends_on:
      redis:
        condition: service_healthy
    restart: always
    deploy:
      replicas: 4
      resources:
        limits:
          cpus: '2'
          memory: 3G
        reservations:
          cpus: '1'
          memory: 1.5G
      update_config:
        parallelism: 2
        delay: 10s
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
    networks:
      - mp4totext_prod
    logging:
      driver: "json-file"
      options:
        max-size: "20m"
        max-file: "5"

  # =============================================================================
  # CELERY WORKER - LOW QUEUE (Cleanup, Maintenance)
  # Scale: 1 replica (sufficient for background tasks)
  # =============================================================================
  celery_worker_low:
    build:
      context: .
      dockerfile: Dockerfile.prod
    command: >
      celery -A app.celery_app worker
      -Q low
      -n worker_low@%h
      --loglevel=warning
      --autoscale=4,1
      --max-tasks-per-child=2000
    environment:
      - ENVIRONMENT=production
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - PYTHONUNBUFFERED=1
    env_file:
      - .env.production
    depends_on:
      redis:
        condition: service_healthy
    restart: always
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    networks:
      - mp4totext_prod
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # =============================================================================
  # CELERY BEAT - Periodic Tasks Scheduler
  # =============================================================================
  celery_beat:
    build:
      context: .
      dockerfile: Dockerfile.prod
    command: celery -A app.celery_app beat --loglevel=warning
    environment:
      - ENVIRONMENT=production
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - PYTHONUNBUFFERED=1
    env_file:
      - .env.production
    depends_on:
      - redis
    restart: always
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    networks:
      - mp4totext_prod
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # =============================================================================
  # FLOWER - Celery Monitoring UI (Production)
  # =============================================================================
  flower:
    build:
      context: .
      dockerfile: Dockerfile.prod
    command: >
      celery -A app.celery_app flower
      --port=5555
      --broker=redis://:${REDIS_PASSWORD}@redis:6379/0
      --basic-auth=${FLOWER_USER}:${FLOWER_PASSWORD}
      --max-tasks=10000
    ports:
      - "5555:5555"
    environment:
      - ENVIRONMENT=production
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
    depends_on:
      redis:
        condition: service_healthy
    restart: always
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    networks:
      - mp4totext_prod
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

volumes:
  redis_prod_data:
    driver: local
  upload_temp:
    driver: local
  model_cache:
    driver: local

networks:
  mp4totext_prod:
    driver: bridge

# =============================================================================
# PRODUCTION DEPLOYMENT NOTES:
# =============================================================================
# 
# Total Worker Capacity:
# - Critical: 3 replicas × 6 workers = 18 concurrent critical tasks
# - High: 5 replicas × 12 workers = 60 concurrent transcriptions
# - Default: 4 replicas × 10 workers = 40 AI enhancements
# - Low: 1 replica × 4 workers = 4 maintenance tasks
# 
# TOTAL: 122 concurrent workers for 1000+ users
# 
# Commands:
# - Start: docker-compose -f docker-compose.prod.yml up -d
# - Scale specific service: docker-compose -f docker-compose.prod.yml up -d --scale celery_worker_high=10
# - View logs: docker-compose -f docker-compose.prod.yml logs -f celery_worker_high
# - Monitor: http://localhost:5555 (Flower UI)
# 
# Environment Variables Required (.env.production):
# - REDIS_PASSWORD
# - FLOWER_USER
# - FLOWER_PASSWORD
# - All API keys (GEMINI_API_KEY, OPENAI_API_KEY, etc.)
# =============================================================================
